{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b62847c",
   "metadata": {},
   "source": [
    "# Scientific Abstract Text Generation -- Word Level\n",
    "\n",
    "In a previous project, I have demonstrated the use of an LSTM network to learn scientific text from PubMed abstracts and generate texts to emulate academic writing [link](https://github.com/chauvu/chauvu.github.io/blob/main/Notebooks/manuscripts_text_generation.ipynb). The previous project performs **character-level** prediction, which means the network receives a sequence of 100 characters and aims to predict the following *character*. The advantage of this method is that there are only <100 possible alphanumeric characters, so the fully-connected layer at the end performs a classification and chooses the character with the highest softmax likelihood. The disadvantage is that without a large training set, the network will generate gibberish characters that do not form coherent words.\n",
    "\n",
    "In this current work, I will be implementing **word-level** text generation, which means the LSTM network predicts which *word* follows a sequence of 100 words. The advantage of this network is that each word is a correct English word. However, the disadvantage is that there are so many possible words to choose from for the classification fully-connected layer, which means that it is difficult to generate a coherent phrase as well. One notable feature of this LSTM network will be an added **attention** mechanism, which is simply a vector of length 100 (same length as our sequence), that we will use to perform a dot product with the LSTM output sequence before performing the prediction.\n",
    "\n",
    "To correct for this, I will be implement **beam search**, which is a recursive NLP algorithm; instead of greedily choosing the next word, beam search keeps track of the softmax score for the last *k* steps and chooses the k-length sequence that has the highest score. Usually `k` ranges between 5 and 10, but in this work I will use `k=3` due to limited time and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23fc87f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import model_selection\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, LSTM, SimpleRNN\n",
    "from tensorflow.keras.layers import Attention, Multiply, LayerNormalization, TimeDistributed\n",
    "from tensorflow.keras.layers import Activation, Permute, Lambda, RepeatVector\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a6a96d",
   "metadata": {},
   "source": [
    "Data scraped from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) is in this [file](https://github.com/chauvu/chauvu.github.io/blob/main/Data/pubmed/abstracts_str.txt). Let's read in this data and perform some basic cleaning. Ideally, I would have performed lemmatization using NLP libraries such as NLTK or SpaCy, but this step was skipped in this project.\n",
    "\n",
    "I will create 100-word long sequences and train my LSTM network on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9701da",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"abstracts_str.txt\"\n",
    "abstract_file = open(filename, 'r', encoding='utf-8')\n",
    "abstract_txt = abstract_file.read()\n",
    "abstract_txt = abstract_txt.lower()\n",
    "abstract_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa31a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into word-level tokens\n",
    "abstract_txt = re.sub(r'[^a-zA-Z0-9_\\s]', ' ', abstract_txt) # clean text\n",
    "abstract_txt = abstract_txt.replace('\\n', ' \\n ')\n",
    "abstract_wordvec = abstract_txt.split(' ')\n",
    "abstract_wordvec = [w for w in abstract_wordvec if len(w)>0] # delete empty\n",
    "\n",
    "# remove out-of-vocab words (< 10 occurences)\n",
    "word_count = {}\n",
    "unique_words = sorted(set(abstract_wordvec))\n",
    "for word in unique_words:\n",
    "\tword_count[word] = abstract_wordvec.count(word)\n",
    "word_to_remove = [w for w in word_count if word_count[w]<10]\n",
    "for w in word_to_remove: # delete word\n",
    "\tabstract_txt = abstract_txt.replace(' '+w+' ', ' ')\n",
    "abstract_wordvec = abstract_txt.split(' ')\n",
    "abstract_wordvec = [w for w in abstract_wordvec if len(w)>0] # delete empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb5bb8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "sequences = []\n",
    "pred_words = []\n",
    "\n",
    "for i in range(0, len(abstract_wordvec)-seq_len):\n",
    "\tsequence = abstract_wordvec[i:i+seq_len]\n",
    "\tpred_word = abstract_wordvec[i+seq_len]\n",
    "\tif('\\n' not in sequence and '\\n'!=pred_word):\n",
    "\t\tsequences.append(sequence)\n",
    "\t\tpred_words.append(pred_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fac3a64",
   "metadata": {},
   "source": [
    "Let's now tokenize the 100-word sequences. Each word will be represented as a specific integer value. I will also create dictionaries `word2index_dict` and `index2word_dict` to easily convert between natural language words and their index values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "215a36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "word_list = sorted(set(abstract_wordvec)) # unique words\n",
    "word_list = word_list[1:] # remove \\n newline\n",
    "index_list = np.arange(len(word_list))\n",
    "word2index_dict = dict(zip(word_list,index_list))\n",
    "index2word_dict = dict(zip(index_list,word_list))\n",
    "\n",
    "sequences_word_list = [] # list of integer arrays\n",
    "pred_word_list = [] # list of integers\n",
    "for i in range(0,len(sequences)):\n",
    "\tseq = sequences[i]\n",
    "\tpred_word = pred_words[i]\n",
    "\tseq_word = [word2index_dict[word] for word in list(seq)]\n",
    "\tsequences_word_list.append(seq_word)\n",
    "\tpred_word_list.append(word2index_dict[pred_word])\n",
    "\n",
    "sequences_word_array = np.array(sequences_word_list)\n",
    "sequences_word_array = np.reshape(sequences_word_array, (len(sequences_word_array),seq_len,1)) # X\n",
    "pred_word_list_onehot = tf.keras.utils.to_categorical(np.array(pred_word_list)) # y\n",
    "\n",
    "# normalize X\n",
    "sequences_word_array_norm = sequences_word_array / (len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a408f49",
   "metadata": {},
   "source": [
    "To simplify the process of converting a whole sequence of words to indices, and vice versa, I will now create two functions `convert_index_to_wordseq` and `convert_word_to_indexseq`. These functions will be used throughout this project when training and performing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a11403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_index_to_wordseq(index_list): # pass in list of integers\n",
    "\twordseq = [index2word_dict[index] for index in index_list]\n",
    "\treturn wordseq\n",
    "\n",
    "def convert_word_to_indexseq(word_list): # pass in list of chars\n",
    "\tindexseq = [word2index_dict[word] for word in word_list]\n",
    "\treturn indexseq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e4057",
   "metadata": {},
   "source": [
    "Here is a test 100-word sentence that I will be using to test the LSTM network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9e7aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Few epidemiologic studies have examined the role of maternal iron status in allergic ' \\\n",
    "'diseases in offspring and findings have been inconsistent. We used a large birth cohort in Japan ' \\\n",
    "'to explore the association of the markers for maternal iron status (maternal hemoglobin, hematocrit ' \\\n",
    "'and dietary iron intake during pregnancy) with allergy development in offspring during early ' \\\n",
    "'childhood. We analyzed information on children age 0–3 years from the Japan Environment and ' \\\n",
    "'Children’s Study (JECS). We used logistic models and generalized estimating equation models to ' \\\n",
    "'evaluate the effect of maternal hemoglobin and hematocrit levels and dietary iron intake on ' \\\n",
    "'allergies in children. Models were also fitted with propensity score-matched datasets. Data were ' \\\n",
    "'collected for a total of 91,247 mother–child pairs. The prevalence (95 confidence interval) of ' \\\n",
    "'low hemoglobin and hematocrit was 14.0% (13.7–14.2%) and 12.5% (12.3–12.8%), respectively. After ' \\\n",
    "'adjusting confounders, low hemoglobin and hematocrit during pregnancy were not associated with ' \\\n",
    "'childhood allergic outcomes. Findings from models with propensity score-matched datasets also ' \\\n",
    "'indicated that children born to mothers with low hemoglobin or hematocrit levels during pregnancy ' \\\n",
    "'did not have a higher risk of developing allergic conditions at 3 years old. We found no meaningful ' \\\n",
    "'associations between low energy adjusted maternal dietary iron intake and allergies in children. In ' \\\n",
    "'conclusion, using birth cohort data, we found no evidence supporting an association of low maternal ' \\\n",
    "'hemoglobin, hematocrit and low dietary iron intake with allergy symptoms during early childhood. ' \\\n",
    "'Further studies with more suitable proxy markers for blood iron status are needed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e3450",
   "metadata": {},
   "source": [
    "Next, for predictions, I will create two separate functions:\n",
    "* `text_generation_random`: baseline function that generates a random 100-word sequence. Since input is gibberish we expect output to be gibberish as well.\n",
    "* `text_generation_sentence`: input is a 100-word coherent sequence (such as the example sentence above), and we expect to generate subsequent words that make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3503d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generation_random(model):\n",
    "    start = np.random.randint(0, len(sequences_word_array_norm)-1)\n",
    "    pattern = sequences_word_list[start]\n",
    "    seq = convert_index_to_wordseq(pattern)\n",
    "    print(' '.join(seq))\n",
    "\n",
    "    for i in range(10):\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = x / float(len(word_list))\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        result = convert_index_to_wordseq([index])\n",
    "        seq += result\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "        print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfc9f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generation_sentence(seq, model):\n",
    "\t# clean sentence\n",
    "\tseq = seq.lower()\n",
    "\tseq = re.sub(r'[^a-zA-Z0-9_\\s]', ' ', seq)\n",
    "\tseq = seq.replace('\\n', ' \\n ')\n",
    "\tfor w in word_to_remove: # delete word\n",
    "\t\tseq = seq.replace(' '+w+' ', ' ')\n",
    "\tseq_wordvec = seq.split(' ')\n",
    "\tseq_wordvec = [w for w in seq_wordvec if len(w)>0]\n",
    "\tseq_wordvec = [w for w in seq_wordvec if w in word_list]\n",
    "\tseq_wordvec = seq_wordvec[:100]\n",
    "\tpattern = convert_word_to_indexseq(seq_wordvec)\n",
    "\tprint(' '.join(seq_wordvec))\n",
    "\n",
    "\tfor i in range(20):\n",
    "\t\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\t\tx = x / float(len(word_list))\n",
    "\t\tprediction = model.predict(x, verbose=0)\n",
    "\t\tindex = np.argmax(prediction)\n",
    "\t\tresult = convert_index_to_wordseq([index])\n",
    "\t\tseq_wordvec += result\n",
    "\t\tpattern.append(index)\n",
    "\t\tpattern = pattern[1:len(pattern)]\n",
    "\t\tprint(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55027dca",
   "metadata": {},
   "source": [
    "# LSTM networks\n",
    "In this project, I will implement 2 LSTM networks and train for fixed 100 epochs. The checkpoints were already generated for convenience.\n",
    "* vanilla 1-layer LSTM: includes an LSTM layer followed by a Dense layer with a softmax activation function for classification.\n",
    "* 1-layer LSTM with self-attention module: includes the vanilla LSTM network. Additionally, there is an `Attention` layer from keras that generates an `attention_scores` vector. This vector will determine how much weight is assigned to a position in the sequence, to determine which word will be more influential on the prediction. This vector `attention_scores` is will be dot-product with the output of the LSTM layer, before being passed into the Dense layer for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf398da7",
   "metadata": {},
   "source": [
    "### 1. Vanilla LSTM\n",
    "Below we see the structure of our vanilla LSTM network, with an LSTM layer, followed by a Dropout layer and a Dense layer with softmax activation.\n",
    "\n",
    "Taking a look at our baseline random results, we see quite a coherent generated sequence `developed symptoms fatigue during`, which is very encouraging. However, when we take a look at the sequence generated from our example sentence, it doesn't seem very understable. The only encouraging part is `in individuals developing the`, and then it seems to falls into a loop with infinite `and`s. It is possible that our LSTM network is too simple (only 1 layer) and has only been trained for 100 epochs on only a small amount of pubmed text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8f5f5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LSTM 256, learning rate 0.001\n",
    "already_generated = True\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(sequences_word_array_norm.shape[1], sequences_word_array_norm.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(pred_word_list_onehot.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001))\n",
    "\n",
    "if not already_generated:\n",
    "\tfilepath=\"checkpoints.nosync/abstracts-lstm256-adam001-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "\tcheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\tcallbacks_list = [checkpoint]\n",
    "\tmodel.fit(sequences_word_array_norm, pred_word_list_onehot, epochs=100, batch_size=128, callbacks=callbacks_list)\n",
    "else:\n",
    "\tfilepath='checkpoints.nosync/abstracts-lstm256-adam001-100-1.8758.hdf5'\n",
    "\tmodel.load_weights(filepath)\n",
    "    \n",
    "model_vanilla = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01813064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of 44 goats affected with pt and 10 healthy goats control group or were used in the study in goats with pt the serum concentrations of 0 ml were significantly higher than that in goats 0 ml although showed no significant difference it was three higher in with pt the serum concentrations of insulin were significantly lower in pt goats 5 03 l compared to goats 10 l the serum concentrations of in pt goats l were significantly higher than that in goats 36 l results of this study indicate that a clinically significant damage might in goats affected with\n",
      "pt\n",
      "in\n",
      "significant\n",
      "in\n",
      "the\n",
      "and\n",
      "with\n",
      "elements\n",
      "of\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "text_generation_random(model_vanilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d462b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few studies have examined the role of maternal status in diseases in offspring and findings have been we used a large cohort in to explore the association of the markers for maternal status maternal and intake during with development in offspring during early childhood we analyzed information on children age 0 3 years from the environment and children s study we used models and models to evaluate the effect of maternal and levels and intake on in children models were also with score matched data were collected for a total of child the prevalence 95 confidence interval of low and\n",
      "in\n",
      "in\n",
      "individuals\n",
      "developing\n",
      "the\n",
      "with\n",
      "and\n",
      "a\n",
      "and\n",
      "95\n",
      "for\n",
      "and\n",
      "the\n",
      "and\n",
      "baseline\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n"
     ]
    }
   ],
   "source": [
    "text_generation_sentence(sentence, model_vanilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca9bd3",
   "metadata": {},
   "source": [
    "### 2. LSTM with Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49cb6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with Self-Attention with Attention Module (dot product)\n",
    "already_generated = True\n",
    "model_type = 'self'\n",
    "\n",
    "input_layer = Input(shape=(sequences_word_array_norm.shape[1], sequences_word_array_norm.shape[2]))\n",
    "lstm_layer = LSTM(256, return_sequences=True)(input_layer)\n",
    "dropout_layer = Dropout(0.2)(lstm_layer)\n",
    "attention_layer, attention_scores = Attention(name='attention_vector')([lstm_layer, lstm_layer], return_attention_scores=True)\n",
    "context_layer = Multiply()([dropout_layer, attention_layer])\n",
    "context_layer = Lambda(lambda x: K.sum(x, axis=1))(context_layer)\n",
    "output_layer = Dense(pred_word_list_onehot.shape[1], activation='softmax')(context_layer)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy')\n",
    "\n",
    "if not already_generated:\n",
    "\tfilepath=\"checkpoints.nosync/abstracts-lstm256-adam001-attention-self-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "\tcheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\tcallbacks_list = [checkpoint]\n",
    "\tmodel.fit(sequences_word_array_norm, pred_word_list_onehot, epochs=100, batch_size=128, callbacks=callbacks_list)\n",
    "else:\n",
    "\tfilepath = 'checkpoints.nosync/abstracts-lstm256-adam001-attention-self-100-2.5492.hdf5'\n",
    "\tmodel.load_weights(filepath)\n",
    "    \n",
    "model_prediction = Model(inputs=model.input, outputs=[model.output, model.get_layer('attention_vector').output])\n",
    "model_attention = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c580cc",
   "metadata": {},
   "source": [
    "Let's generate the text pattern and look at our attention vector. Our attention vector is visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9cc890c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few studies have examined the role of maternal status in diseases in offspring and findings have been we used a large cohort in to explore the association of the markers for maternal status maternal and intake during with development in offspring during early childhood we analyzed information on children age 0 3 years from the environment and children s study we used models and models to evaluate the effect of maternal and levels and intake on in children models were also with score matched data were collected for a total of child the prevalence 95 confidence interval of low and\n"
     ]
    }
   ],
   "source": [
    "def text_generation_sentence_attention(seq, model, model_type='self'):\n",
    "\t# clean sentence\n",
    "\tseq = seq.lower()\n",
    "\tseq = re.sub(r'[^a-zA-Z0-9_\\s]', ' ', seq)\n",
    "\tseq = seq.replace('\\n', ' \\n ')\n",
    "\tfor w in word_to_remove: # delete word\n",
    "\t\tseq = seq.replace(' '+w+' ', ' ')\n",
    "\tseq_wordvec = seq.split(' ')\n",
    "\tseq_wordvec = [w for w in seq_wordvec if len(w)>0]\n",
    "\tseq_wordvec = [w for w in seq_wordvec if w in word_list]\n",
    "\tseq_wordvec = seq_wordvec[:100]\n",
    "\tpattern = convert_word_to_indexseq(seq_wordvec)\n",
    "\tprint(' '.join(seq_wordvec))\n",
    "\n",
    "\tfor i in range(1): # visualize 1 time step\n",
    "\t\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\t\tx = x / float(len(word_list))\n",
    "\t\tprediction, attention = model.predict(x, verbose=0)\n",
    "\t\tindex = np.argmax(prediction)\n",
    "\t\tresult = convert_index_to_wordseq([index])\n",
    "\t\tseq_wordvec += result\n",
    "\t\tpattern.append(index)\n",
    "\t\tpattern = pattern[1:len(pattern)]\n",
    "\n",
    "\tif model_type == 'self':\n",
    "\t\tw = attention[1]\n",
    "\t\tw = np.mean(w.squeeze(), axis=0)\n",
    "\telse:\n",
    "\t\tw = attention\n",
    "\tw = np.reshape(w, (1,100))\n",
    "\treturn w\n",
    "\n",
    "\n",
    "attention_weights = text_generation_sentence_attention(sentence, model_prediction, model_type=model_type)\n",
    "w = attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b90eae20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_attention_weights(w, seq_wordvec, indices):\n",
    "\tplt.figure(figsize=(15, 6))\n",
    "\tax = plt.gca()\n",
    "\n",
    "\tseq_wordvec_indices = [seq_wordvec[i] for i in indices]\n",
    "\tw_indices = np.array([w[0,i] for i in indices])\n",
    "\n",
    "\thorizontal_rep = 10\n",
    "\tw_indices = np.repeat(w_indices, horizontal_rep, axis=0)\n",
    "\tw_indices = np.reshape(w_indices, (1, horizontal_rep*len(indices)))\n",
    "\tw_indices = np.repeat(w_indices, 20, axis=0)\n",
    "\tax.imshow(w_indices)\n",
    "\t\n",
    "\n",
    "\t# ax.set_xticks(np.arange(len(indices)))\n",
    "\tax.set_xticks(np.linspace(0, (len(indices)-1)*horizontal_rep, len(indices)) + horizontal_rep/2)\n",
    "\tax.set_xticklabels(seq_wordvec_indices)\n",
    "\tplt.show()\n",
    "\n",
    "visualize_attention_weights(w, convert_word_to_indexseq(seq), np.arange(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab692968",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "In this part, I will implement **beam search**, which is a recursive and greedy algorithm that not only choose the next word prediction with the highest softmax score, but chooses the next k-length sequence with the highest score. In this work, I will choose `k=3` for simplification. The softmax score for the whole 3-word sequence is calculated as the logsum of the score of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cadb63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_generated = 10 # generate 10 words after\n",
    "def text_generation_beam_search_sentence(seq, model, beam_search_k, length_generated):\n",
    "\t# clean sentence\n",
    "\tseq = seq.lower()\n",
    "\tseq = re.sub(r'[^a-zA-Z0-9_\\s]', ' ', seq)\n",
    "\tseq = seq.replace('\\n', ' \\n ')\n",
    "\tfor w in word_to_remove: # delete word\n",
    "\t\tseq = seq.replace(' '+w+' ', ' ')\n",
    "\tseq_wordvec = seq.split(' ')\n",
    "\tseq_wordvec = [w for w in seq_wordvec if len(w)>0]\n",
    "\tseq_wordvec = [w for w in seq_wordvec if w in word_list]\n",
    "\tseq_wordvec = seq_wordvec[:100]\n",
    "\tpattern = convert_word_to_indexseq(seq_wordvec)\n",
    "\tprint(' '.join(seq_wordvec))\n",
    "\n",
    "\tmax_proba, last_word, generated_sequence = beam_search_recursion(pattern, 0, model, beam_search_k, length_generated)\n",
    "\tprint(convert_index_to_wordseq(generated_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f549747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_recursion(pattern, previous_proba, model, beam_search_k, length_generated):\n",
    "\t# print('Length: '+str(length_generated))\n",
    "\tif length_generated == 0: # no more words to generate\n",
    "\t\treturn previous_proba, 0, [0]\n",
    "\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(len(word_list))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tsorted_index = np.argsort(-prediction) # sorted reverse\n",
    "\tbeam_search_index = sorted_index[0,:beam_search_k]\n",
    "\tbeam_search_proba = prediction[0, beam_search_index] # probabilities\n",
    "\n",
    "\tprobas = []\n",
    "\tbest_indices = []\n",
    "\tfor k in range(beam_search_k):\n",
    "\t\tproba = previous_proba + np.log(beam_search_proba[k])\n",
    "\t\tproba_k, best_index, full_pattern = beam_search_recursion(pattern[1:] + [beam_search_index[k]], \n",
    "\t\t\tproba, model, beam_search_k, length_generated-1)\n",
    "\t\tprobas.append(proba_k)\n",
    "\t\tbest_indices.append(best_index)\n",
    "\tmax_proba = np.max(probas)\n",
    "\tfull_pattern = [beam_search_index[np.argmax(probas)]] + full_pattern\n",
    "\n",
    "\treturn max_proba, beam_search_index[np.argmax(probas)], full_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29297a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few studies have examined the role of maternal status in diseases in offspring and findings have been we used a large cohort in to explore the association of the markers for maternal status maternal and intake during with development in offspring during early childhood we analyzed information on children age 0 3 years from the environment and children s study we used models and models to evaluate the effect of maternal and levels and intake on in children models were also with score matched data were collected for a total of child the prevalence 95 confidence interval of low and\n",
      "['in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'differences', '0']\n"
     ]
    }
   ],
   "source": [
    "text_generation_beam_search_sentence(sentence, model_attention, 3, 10) # length_generated = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29ee59f",
   "metadata": {},
   "source": [
    "The generated text is exactly the same as non-beam search result from LSTM because each word has the same weight in the 100-word sequence. Each new word does not have as much weight as previous 99 words, so changing the last word does not affect the next generated word. It might be better to perform beam search with a large `k` and put more weight on the newer generated words compared to the previous ~97-word sequence.\n",
    "\n",
    "Currently, for evaluation, I am using only visual assessment of the attention module. Another option is to use criteria such as BLEU (similarity to a sentence), but since our generated text is not very good the BLEU score is guaranteed to be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03e748bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_generated = 10\n",
    "def text_generation_random_BLEU_score(model):\n",
    "    start = np.random.randint(0, len(abstract_wordvec)-length_generated)\n",
    "    sequence = abstract_wordvec[start:start+100*2]\n",
    "    seq = ' '.join(sequence)\n",
    "\n",
    "    seq = seq.lower()\n",
    "    seq = re.sub(r'[^a-zA-Z0-9_\\s]', ' ', seq)\n",
    "    seq = seq.replace('\\n', ' \\n ')\n",
    "    for w in word_to_remove: # delete word\n",
    "        seq = seq.replace(' '+w+' ', ' ')\n",
    "    seq_wordvec = seq.split(' ')\n",
    "    seq_wordvec = [w for w in seq_wordvec if len(w)>0]\n",
    "    seq_wordvec = [w for w in seq_wordvec if w in word_list]\n",
    "    seq_wordvec_sentence = seq_wordvec[:100]\n",
    "    seq_wordvec_truth = seq_wordvec[100:100+length_generated]\n",
    "    pattern = convert_word_to_indexseq(seq_wordvec_sentence)\n",
    "    truth = convert_word_to_indexseq(seq_wordvec_truth)\n",
    "    seq = seq_wordvec_sentence\n",
    "\n",
    "    generated = []\n",
    "    print(' '.join(seq_wordvec))\n",
    "\n",
    "    for i in range(length_generated):\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = x / float(len(word_list))\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        result = convert_index_to_wordseq([index])\n",
    "        seq += result\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "        print(result[0])\n",
    "        generated.append(result[0])\n",
    "    BLEU_score = sentence_bleu(seq_wordvec_truth, ' '.join(generated))\n",
    "    print(BLEU_score)\n",
    "\n",
    "    return seq_wordvec_truth, generated, BLEU_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55831b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biological trial was conducted on day old n  which were into 6 groups 8 with 5 six experimental were to diet i qpm diet ii kg diet iii kg diet iv qpm kg diet v and qpm kg diet the and intake were at supplementation of csm and with qpm or depressed p   0 05 ratio and variables in improvement of and breast was among the groups kg csm and kg with a qpm based diet compared with those groups kg csm and kg with or qpm the energy and protein utilization decreased among the groups csm and however protein and energy utilization was increased p   0 05 among the groups qpm based compared with those groups csm and with or qpm therefore it has been that the performance and other parameters did not differ between the groups and qpm based in the present experiment however qpm with csm and improved the performance parameters and nutrient utilization over csm and with are one of the most common that are by many including infection severe inflammatory response expression of and formation the aim of this study was to investigate the effect of type a bo and ap or in combination\n",
      "utilization\n",
      "energy\n",
      "utilization\n",
      "among\n",
      "utilization\n",
      "groups\n",
      "among\n",
      "among\n",
      "among\n",
      "groups\n",
      "0.208063911483483\n"
     ]
    }
   ],
   "source": [
    "results = text_generation_random_BLEU_score(model_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b198c1",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "For this work we created a LSTM network with self-attention module and a beam search algorithm to perform text generation for medical abstracts on PubMed. The results are not ideal, since the LSTM network is small (1 layer) and we don't have a lot of training data (only 300 short abstracts). If we have more training data and use a more complex network (maybe transformer BERT), then it is possible to perform better. Overall, the beam search and attention module can improve performance, but we will need a robust deep learning network as base in order to do well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af9945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
