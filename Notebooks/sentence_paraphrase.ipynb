{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cda5ada",
   "metadata": {},
   "source": [
    "# Sentence Paraphrase using HuggingFace.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed825f74",
   "metadata": {},
   "source": [
    "In this project, I will use the Facebook Large BART model to train a network to paraphrase sentences. The data was downloaded and can be accessed in the `data/` folder. The pre-trained BART model is provided by huggingface.co and accessed through the `simpletransformers` library in python. \n",
    "\n",
    "This project is fairly simple and the BART network will be used as part of a larger NLP project to generate new sentences by paraphrasing. Even though the downstream task is more complex, this script illustrates the paraphraser training and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8ad3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "already_trained = True\n",
    "input_folder = '../Data/'\n",
    "output_folder = '../Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ad615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = input_folder + 'train.tsv'\n",
    "eval_file = input_folder + 'dev.tsv'\n",
    "train_df = pd.read_csv(train_file, sep=\"\\t\").astype(str)\n",
    "eval_df = pd.read_csv(eval_file, sep=\"\\t\").astype(str)\n",
    "train_df = train_df.loc[train_df['label'] == '1']\n",
    "eval_df = eval_df.loc[eval_df['label'] == '1']\n",
    "train_df = train_df.rename(columns={\"sentence1\": \"input_text\", \"sentence2\": \"target_text\"})\n",
    "eval_df = eval_df.rename(columns={\"sentence1\": \"input_text\", \"sentence2\": \"target_text\"})\n",
    "train_df = train_df[['input_text', 'target_text']]\n",
    "eval_df = eval_df[['input_text', 'target_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5712ee7",
   "metadata": {},
   "source": [
    "Let's load in the data from our train and dev tsv files. These dataframes have been renamed to have 2 columns `input_text` and `target_text`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a787971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The NBA season of 1975 -- 76 was the 30th seas...</td>\n",
       "      <td>The 1975 -- 76 season of the National Basketba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When comparable rates of flow can be maintaine...</td>\n",
       "      <td>The results are high when comparable flow rate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It is the seat of Zerendi District in Akmola R...</td>\n",
       "      <td>It is the seat of the district of Zerendi in A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>William Henry Henry Harman was born on 17 Febr...</td>\n",
       "      <td>William Henry Harman was born in Waynesboro , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>With a discrete amount of probabilities Formul...</td>\n",
       "      <td>Given a discrete set of probabilities formula ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text  \\\n",
       "1  The NBA season of 1975 -- 76 was the 30th seas...   \n",
       "3  When comparable rates of flow can be maintaine...   \n",
       "4  It is the seat of Zerendi District in Akmola R...   \n",
       "5  William Henry Henry Harman was born on 17 Febr...   \n",
       "7  With a discrete amount of probabilities Formul...   \n",
       "\n",
       "                                         target_text  \n",
       "1  The 1975 -- 76 season of the National Basketba...  \n",
       "3  The results are high when comparable flow rate...  \n",
       "4  It is the seat of the district of Zerendi in A...  \n",
       "5  William Henry Harman was born in Waynesboro , ...  \n",
       "7  Given a discrete set of probabilities formula ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2786905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21829, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1049cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3539, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4152b6c4",
   "metadata": {},
   "source": [
    "The train set consists of >20k pairs of sentences, whereas the test set includes 3.5k pairs. Below is an example of a pair of sentences. We see that the paraphrased sentences (in this case and in most of the pairs in our data) are simple positional reorganization of phrases: the paraphrased sentences generally do not consist of out-of-bag words or synonyms for words in the input text. The text is simply shuffled while keeping the meaning the same.\n",
    "\n",
    "While this paraphrasing is very simple, it is exactly what is needed for my downstream task. If we wanted to do more complex paraphrasing, in which synonyms are used for example, a different and larger dataset is required to adequately train this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82cc1638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0]['input_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dffdc8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0]['target_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0e0cf0",
   "metadata": {},
   "source": [
    "Here are the model arguments required for BART by `simpletransformers`. We will train it for 50 epochs and train it on a GPU. BART is a very large model, so even though we only have 20k sentences, the model still trains for a whole day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68e41e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = Seq2SeqArgs()\n",
    "model_args.output_dir = output_folder\n",
    "model_args.num_train_epochs = 50\n",
    "model_args.no_save = False\n",
    "model_args.use_multiprocessing = False\n",
    "model_args.evaluate_generated_text = True\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee548ac",
   "metadata": {},
   "source": [
    "Since I've already trained this BART model, for this script I will not run it again and instead will load it in from my checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "239c2472",
   "metadata": {},
   "outputs": [],
   "source": [
    "if already_trained:\n",
    "    model2 = Seq2SeqModel(\n",
    "        encoder_decoder_type = \"bart\",\n",
    "        encoder_decoder_name = output_folder + 'checkpoint_epoch_50/',\n",
    "        args = model_args,\n",
    "        use_cuda = False\n",
    "        )\n",
    "else:\n",
    "    model = Seq2SeqModel(\n",
    "        encoder_decoder_type = \"bart\",\n",
    "        encoder_decoder_name = \"facebook/bart-large\",\n",
    "        args = model_args,\n",
    "        use_cuda = True\n",
    "        )\n",
    "    model.train_model(train_df, eval_data=eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd325942",
   "metadata": {},
   "source": [
    "Likewise, the predictions have already been generated, so I will load it in from the pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57773b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = eval_df['input_text'].tolist()\n",
    "truth = eval_df[\"target_text\"].tolist()\n",
    "\n",
    "if already_trained:\n",
    "    preds_df = pd.read_pickle(output_folder + 'predictions.pkl')\n",
    "    preds = preds_df['predictions'].tolist()\n",
    "else:\n",
    "    preds = model.predict(to_predict)\n",
    "    preds_df = pd.DataFrame(preds, columns=['predictions'])\n",
    "    preds_df.to_pickle(output_folder + 'predictions.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf764f",
   "metadata": {},
   "source": [
    "Now let's take look at the results. In example #1, the prediction is a shuffled paraphrasing of the input text. The prediction is not exactly the same as the truth, but the prediction is still grammatically correct, makes semantic sense and is not an exact copy of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee389b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 1\n",
      "INPUT They were there to enjoy us and they were there to pray for us .\n",
      "TRUTH They were there for us to enjoy and they were there for us to pray .\n",
      "PREDS They were there to enjoy us and they were there for us to pray.\n"
     ]
    }
   ],
   "source": [
    "print('EXAMPLE 1')\n",
    "print('INPUT', to_predict[0])\n",
    "print('TRUTH', truth[0])\n",
    "print('PREDS', preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ac6143",
   "metadata": {},
   "source": [
    "In this case, the prediction did not perform exceptionally well, since the prediction is extremely similar to the input sentence. The only difference were the exclusion of 2 punctuations (`,` and `.`). This demonstrates that possibly cleaning the sentences before training, namely removing punctuations, can potentially improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4403b53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 2\n",
      "INPUT The group toured extensively and became famous in Israel , and even played in New York City in 2007 .\n",
      "TRUTH The group toured extensively and was famous in Israel and even played in New York City in 2007 .\n",
      "PREDS The group toured extensively and became famous in Israel and even played in New York City in 2007\n"
     ]
    }
   ],
   "source": [
    "print('EXAMPLE 2')\n",
    "print('INPUT', to_predict[3])\n",
    "print('TRUTH', truth[3])\n",
    "print('PREDS', preds[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80f1df18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 3\n",
      "INPUT From the west end of the bridge , Pennsylvania Route 268 leads south to Parker and north to Emlenton .\n",
      "TRUTH The Pennsylvania Route 268 leads from the west end of the bridge south to Parker and to the north to Emlenton .\n",
      "PREDS Pennsylvania Route 268 leads from the west end of the bridge south to Parker and north to\n"
     ]
    }
   ],
   "source": [
    "print('EXAMPLE 3')\n",
    "print('INPUT', to_predict[50])\n",
    "print('TRUTH', truth[50])\n",
    "print('PREDS', preds[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b763b36",
   "metadata": {},
   "source": [
    "Overall, the model performs fairly well. Some of the paraphrased sentences are not drastically different from the input texts, and some sentences are not grammatically correct. However, for a large model that has only been trained for 50 epochs, the results are astounding. This small project demonstrates the robustness and potential of transformers in NLP related tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87309297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
