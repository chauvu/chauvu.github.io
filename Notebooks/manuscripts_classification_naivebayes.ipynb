{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of PubMed manuscripts using Naive Bayes\n",
    "\n",
    "Previously, we have scraped 26000 manuscript references from [PubMed](https://pubmed.ncbi.nlm.nih.gov/). The dataset is available in our `Data/pubmed/` [folder](https://github.com/chauvu/chauvu.github.io/tree/main/Data/pubmed).\n",
    "\n",
    "In this project, we want to classify the PubMed references to their topic classes based on the abstract text. The abstract is a concise summary of the manuscript and is provided with almost every manuscript publication. We will use the Naive Bayes method to classify references according to their topics.\n",
    "\n",
    "One barrier to this project is the lack of specific topic classes for the manuscripts. Each manuscript entry contains the title, author list, abstract and list of 5-7 keywords. We will make use of these keywords to group manuscripts into topics ([dataset](https://github.com/chauvu/chauvu.github.io/tree/main/Data/pubmed/manuscripts_topics.pkl)); afterward, we will use Naive Bayes to perform classification of each abstract to their corresponding topic class. Notably, Naive Bayes is implemented in this script instead of using the default Naive Bayes class on `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titles</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Date</th>\n",
       "      <th>PMID</th>\n",
       "      <th>Free Article</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Characteristics of the isocitrate dehydrogenas...</td>\n",
       "      <td>Qu CX, Ji HM, Shi XC, Bi H, Zhai LQ, Han DW.</td>\n",
       "      <td>Brain Behav</td>\n",
       "      <td>2020</td>\n",
       "      <td>32146731</td>\n",
       "      <td>False</td>\n",
       "      <td>OBJECTIVES: To explore the characteristics of ...</td>\n",
       "      <td>Chinese gliomas; IDH mutation; TERT promoter m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Demographics, Natural History and Treatment Ou...</td>\n",
       "      <td>Savage P, Winter M, Parker V, Harding V, Sita-...</td>\n",
       "      <td>BJOG</td>\n",
       "      <td>2020</td>\n",
       "      <td>32146729</td>\n",
       "      <td>False</td>\n",
       "      <td>OBJECTIVE: To investigate the demographics, na...</td>\n",
       "      <td>Choriocarcinoma; chemotherapy; demographics; i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Integrated seed proteome and phosphoproteome a...</td>\n",
       "      <td>Sinha A, Haider T, Narula K, Ghosh S, Chakrabo...</td>\n",
       "      <td>Proteomics</td>\n",
       "      <td>2020</td>\n",
       "      <td>32146728</td>\n",
       "      <td>False</td>\n",
       "      <td>Nutrient dynamics in storage organs is a compl...</td>\n",
       "      <td>2DE; chickpea; mass spectrometry; nutrient; pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is R(+)-Baclofen the best option for the futur...</td>\n",
       "      <td>Echeverry-Alzate V, Jeanblanc J, Sauton P, Blo...</td>\n",
       "      <td>Addict Biol</td>\n",
       "      <td>2020</td>\n",
       "      <td>32146727</td>\n",
       "      <td>False</td>\n",
       "      <td>For several decades, studies conducted to eval...</td>\n",
       "      <td>GABAB receptor; R(+)-Baclofen; RS(±)-Baclofen;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Association between the dimensions of the maxi...</td>\n",
       "      <td>Zhang B, Wei Y, Cao J, Xu T, Zhen M, Yang G, C...</td>\n",
       "      <td>J Periodontol</td>\n",
       "      <td>2020</td>\n",
       "      <td>32146722</td>\n",
       "      <td>False</td>\n",
       "      <td>BACKGROUND: The information of the association...</td>\n",
       "      <td>Cone-beam computed tomography; molars; mucosal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Titles  \\\n",
       "0  Characteristics of the isocitrate dehydrogenas...   \n",
       "1  Demographics, Natural History and Treatment Ou...   \n",
       "2  Integrated seed proteome and phosphoproteome a...   \n",
       "3  Is R(+)-Baclofen the best option for the futur...   \n",
       "4  Association between the dimensions of the maxi...   \n",
       "\n",
       "                                             Authors        Journal  Date  \\\n",
       "0       Qu CX, Ji HM, Shi XC, Bi H, Zhai LQ, Han DW.    Brain Behav  2020   \n",
       "1  Savage P, Winter M, Parker V, Harding V, Sita-...           BJOG  2020   \n",
       "2  Sinha A, Haider T, Narula K, Ghosh S, Chakrabo...     Proteomics  2020   \n",
       "3  Echeverry-Alzate V, Jeanblanc J, Sauton P, Blo...    Addict Biol  2020   \n",
       "4  Zhang B, Wei Y, Cao J, Xu T, Zhen M, Yang G, C...  J Periodontol  2020   \n",
       "\n",
       "       PMID  Free Article                                           Abstract  \\\n",
       "0  32146731         False  OBJECTIVES: To explore the characteristics of ...   \n",
       "1  32146729         False  OBJECTIVE: To investigate the demographics, na...   \n",
       "2  32146728         False  Nutrient dynamics in storage organs is a compl...   \n",
       "3  32146727         False  For several decades, studies conducted to eval...   \n",
       "4  32146722         False  BACKGROUND: The information of the association...   \n",
       "\n",
       "                                            Keywords  \n",
       "0  Chinese gliomas; IDH mutation; TERT promoter m...  \n",
       "1  Choriocarcinoma; chemotherapy; demographics; i...  \n",
       "2  2DE; chickpea; mass spectrometry; nutrient; pr...  \n",
       "3  GABAB receptor; R(+)-Baclofen; RS(±)-Baclofen;...  \n",
       "4  Cone-beam computed tomography; molars; mucosal...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manuscripts = pd.read_csv('../Data/pubmed/manuscripts.csv')\n",
    "manuscripts = manuscripts[manuscripts['Keywords'].notnull()] # drop entries with null keywords\n",
    "manuscripts = manuscripts[manuscripts['Abstract'].notnull()] # drop entries with null abstracts\n",
    "manuscripts.reset_index(drop=True, inplace=True)\n",
    "manuscripts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate topics\n",
    "\n",
    "Each reference entry contains the list of keywords, which are relevant topics of the manuscript. Since a manuscript is a complex piece of writing that can fall into multiple topics, we will assume that these topics are mutually exclusive (which are likely not true, e.g. 'IDH mutation' and 'mutation frequencies' are related). With this assumption, we will account for the frequencies of each topic and **choose the top 10 most popular topics** for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chinese gliomas; IDH mutation; TERT promoter mutation; mutation frequencies; overall survival analysis; sanger sequencing'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manuscripts['Keywords'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Choriocarcinoma; chemotherapy; demographics; incidence'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manuscripts['Keywords'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the frequency table of each topic, we will have to split the `Keywords` columns into a list of keywords (separated by the semi-colon). The top 10 topics of our manuscripts are:\n",
    "\n",
    "   * `Apoptosis` (cell death)\n",
    "   * `Breast cancer`\n",
    "   * `Cancer`\n",
    "   * `Depression`\n",
    "   * `Epidemiology`\n",
    "   * `Inflammation`\n",
    "   * `Obesity`\n",
    "   * `Oxidative stress`\n",
    "   * `Prognosis` (likely course of disease)\n",
    "   * `Quality of life`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apoptosis', 'breast cancer', 'cancer', 'depression', 'epidemiology', 'inflammation', 'obesity', 'oxidative stress', 'prognosis', 'quality of life']\n"
     ]
    }
   ],
   "source": [
    "manuscripts['Keywords_list'] = manuscripts['Keywords'].apply(lambda x: [y.strip().lower() for y in x.split(';')])\n",
    "keywords = [k.strip().lower() for ks in list(manuscripts['Keywords_list']) for k in ks]\n",
    "keywords = pd.Series(keywords)\n",
    "\n",
    "# top 10 topics\n",
    "topics = list(keywords.value_counts().head(10).index)\n",
    "topics.sort()\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these 10 topics, we can clearly see that the topics are *not* mutually exclusive. For example, `breast cancer` and `cancer` are related, and `breast cancer` should fall within the larger topic `cancer`. To address this incorrect assumption, we will check the overlap of manuscripts between different topics.\n",
    "\n",
    "From the frequency of overlaps, `inflammation` and `oxidative stress` have the most overlap, which makes sense because the process of inflammation is typically caused by the generation of reactive oxygen species, leading to oxidative stress. Therefore, I will remove `oxidative stress` as a topic but retain `inflammation`.\n",
    "\n",
    "Additionally, I will also remove `breast cancer` due to the overlap with the `cancer` topic and remove `epidemiology` as a topic due to its broad definition (incidence, distribution and control of diseases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlaps is 1880\n",
      "inflammation, oxidative stress    21\n",
      "breast cancer, prognosis          11\n",
      "apoptosis, oxidative stress        8\n",
      "inflammation, obesity              8\n",
      "epidemiology, obesity              8\n",
      "Name: Overlap_topics, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "overlap = [0] * len(manuscripts)\n",
    "topics_overlap = [''] * len(manuscripts)\n",
    "for index, row in manuscripts.iterrows():\n",
    "    for t in topics:\n",
    "        if t in row['Keywords_list']:\n",
    "            overlap[int(index)] += 1\n",
    "            if topics_overlap[int(index)] == '':\n",
    "                topics_overlap[int(index)] = t\n",
    "            else:\n",
    "                topics_overlap[int(index)] += ', ' + t\n",
    "manuscripts['Overlap_count'] = overlap\n",
    "manuscripts['Overlap_topics'] = topics_overlap\n",
    "\n",
    "# over 1000 manuscripts in these topics\n",
    "print('Number of overlaps is {}'.format(len(manuscripts.loc[manuscripts['Overlap_count']>0])))\n",
    "topics_overlap = manuscripts.loc[manuscripts['Overlap_count']>1, 'Overlap_topics']\n",
    "print(topics_overlap.value_counts()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop these topics\n",
    "topics.remove('oxidative stress')\n",
    "topics.remove('epidemiology')\n",
    "topics.remove('breast cancer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the topics: `apoptosis`, `cancer`, `depression`, `inflammation`, `obesity`, `prognosis` and `quality of life`, we will only perform analysis on the subset of manuscripts within these topics. Additionally, any manuscript reference that contains an overlap of these 7 topics will be removed from the dataset. All columns are removed except for the `Topic` and `Abstract` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "manuscripts = manuscripts.loc[manuscripts['Overlap_count']==1] # only 1 topic\n",
    "manuscripts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "abstract_topic = [' '] * len(manuscripts)\n",
    "for index, row in manuscripts.iterrows():\n",
    "    for t in topics:\n",
    "        if t in row['Keywords_list']:\n",
    "            abstract_topic[int(index)] = t\n",
    "            break\n",
    "abstract_topic = [at.strip() for at in abstract_topic]\n",
    "manuscripts['Topic'] = abstract_topic\n",
    "manuscripts = manuscripts[manuscripts['Topic']!='']\n",
    "manuscripts = manuscripts[['Abstract','Topic']]\n",
    "manuscripts.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BACKGROUND: The purpose of this prospective st...</td>\n",
       "      <td>inflammation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INTRODUCTION: Each dermatological condition as...</td>\n",
       "      <td>quality of life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An abdominal aortic aneurysm (AAA) is a relati...</td>\n",
       "      <td>inflammation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For several years, the number of studies on th...</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neuroblastoma (NB) is the common pediatric tum...</td>\n",
       "      <td>apoptosis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract            Topic\n",
       "0  BACKGROUND: The purpose of this prospective st...     inflammation\n",
       "1  INTRODUCTION: Each dermatological condition as...  quality of life\n",
       "2  An abdominal aortic aneurysm (AAA) is a relati...     inflammation\n",
       "3  For several years, the number of studies on th...       depression\n",
       "4  Neuroblastoma (NB) is the common pediatric tum...        apoptosis"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manuscripts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inflammation       209\n",
      "depression         197\n",
      "apoptosis          196\n",
      "prognosis          186\n",
      "obesity            180\n",
      "quality of life    161\n",
      "cancer             130\n",
      "Name: Topic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(manuscripts['Topic'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we retain 1259 manuscripts. The dataset is slightly unbalanced, with the most popular topic `inflammation` containing 209 entries while the least popular topic `cancer` contain 130 entries. This dataframe is written into a pickle file `manuscripts_topics` in our Data folder for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "manuscripts.to_pickle('../Data/pubmed/manuscripts_topics.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification into topics\n",
    "\n",
    "We start of by cleaning the text data in `Abstract` column, by removing all capitalization, removal of digits and words shorter than 5 characters to retain only *complex* words. We show an example of a processed abstract below, with variable number of spaces between words. Since Naive Bayes uses a *bag of words* approach, the spaces do not matter.\n",
    "\n",
    "Afterward, we split into training and testing set (20% test). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower() and remove all punctuations\n",
    "manuscripts['Abstract'] = manuscripts['Abstract'].str.lower().str.replace(r'[\\W]', ' ')\n",
    "# remove words shorter than 5 characters (only complex words)\n",
    "manuscripts['Abstract'] = manuscripts['Abstract'].str.replace(r'\\b\\w\\w?\\w?\\w?\\b', '')\n",
    "# remove numbers\n",
    "manuscripts['Abstract'] = manuscripts['Abstract'].str.replace(r'\\b\\d+\\b', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'background   purpose   prospective study   compare  changes  periodontal somatosensory function  microcirculation  patients  periodontitis following initial treatment  scaling   planing      without adjuvant laser therapy methods  twenty  patients suffering  periodontitis  recruited  randomly allocated   split mouth design  either  combined laser therapy           control     treatments  performed    investigator   single visit  laser doppler flowmetry     quantitative sensory testing     performed  baseline            weeks       weeks    after treatment   sides   attached gingiva   maxillary lateral incisor  clinical examination including pocket probing depth     bleeding  probing     performed          sides    analyzed    analysis  variance  anova  results      significantly improved after treatment          values  significantly decreased   sides   follow   points        temperature  increased             whereas there   significant change   control          significantly  sensitivity  observed    parameters       except  warmth detection after treatment conclusion  adjunctive   laser therapy   provide  significant clinically advantage  additional effects   recovery  periodontal somatosensory function  gingival microcirculation   present study   article  protected  copyright   rights reserved  article  protected  copyright   rights reserved '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manuscripts.iloc[0]['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(manuscripts, test_size=0.2, random_state=1)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will built the vocabulary space from the training set. Even though there might be extra vocabularies in the testing set, we will assume that the training set represents the entire word space; any word not in the training set does not exist and can be replace with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "vocab_count = {}\n",
    "for index, row in train_df.iterrows():\n",
    "    words = row['Abstract'].split() # split by space\n",
    "    vocab = vocab.union(set(words))\n",
    "    for w in words:\n",
    "        if w in vocab_count:\n",
    "            vocab_count[w] += 1\n",
    "        else:\n",
    "            vocab_count[w] = 1\n",
    "vocab_count_series = pd.Series(vocab_count).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patients      1844\n",
      "study         1566\n",
      "results       1035\n",
      "cancer         790\n",
      "between        759\n",
      "group          733\n",
      "associated     659\n",
      "levels         638\n",
      "treatment      635\n",
      "depression     605\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(vocab_count_series.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chictr            1\n",
      "crocus            1\n",
      "sativus           1\n",
      "saffron           1\n",
      "fourty            1\n",
      "solvents          1\n",
      "piperlongumine    1\n",
      "edible            1\n",
      "pepper            1\n",
      "trace             1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(vocab_count_series.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the frequency table of the vocabularies, we see that the most frequently-used words are general words that likely appear in many abstracts. For example, `study` and `results` are extremely general since each scientific paper is an independent *study* and needs to provide *results* to be published in a journal. On the other hand, the bottom words are very rare; most words such as `chictr` (Chinese clinical trial registry) or `crocus` (a type of iris flower) are so rare most people do not know what they mean.\n",
    "\n",
    "To avoid the overly-general words and the obscure words, we will remove top results with > 400 occurences and bottom results with < 30 occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_count_series = vocab_count_series[vocab_count_series>=30]\n",
    "vocab_count_series = vocab_count_series[vocab_count_series<=400]\n",
    "vocab = set(list(vocab_count_series.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting `vocab` to a list object and sorting it alphabetically, we can look at a subset of words. We can immediately recognize that a lot of words can be combined, such as `addition`, `additional`, and `additionally`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(vocab)\n",
    "vocab_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'about',\n",
       " 'accompanied',\n",
       " 'according',\n",
       " 'accumulation',\n",
       " 'accuracy',\n",
       " 'acids',\n",
       " 'across',\n",
       " 'activated',\n",
       " 'activation',\n",
       " 'active',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'acute',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'adenocarcinoma',\n",
       " 'adherence',\n",
       " 'adjusted']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words such as `additional` and `additionally` should be merged into `addition` since they are simply variant forms of the same word. This process is called **lemmatization**. We do this by creating a dictionary `root_words`, with two keys `root` (the lemmatized root word) and `remove` (conjugation of the root word that should be removed). \n",
    "\n",
    "We assume that the root word and conjugated words are directly adjacent to each other in the alphabetically-sorted list. This assumption is mostly valid, for example plural versions of a noun usually have an additional `-s` at the end (such as `adult` and `adults`), or simple past conjugation of a verb usually have an `-ed` at the end (like `show` and `showed`). Some other exceptions are ignored, such as antonyms like `clear` and `unclear`. After going through the vocab list, I also manually modified a couple words.\n",
    "\n",
    "After word, we removed all the `remove` words from the training and testing datasets as well as the whole vocabulary space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_words = []\n",
    "conjugations = ['s','al','ly','ed','d','ion'] # plural nouns, adjectives, adverbs, past tense\n",
    "for idx in range(len(vocab_list)-1): # compare current word with next word\n",
    "    v1 = vocab_list[idx]\n",
    "    v2 = vocab_list[idx+1]\n",
    "    for conj in conjugations:\n",
    "        if v1 + conj == v2:\n",
    "            # print(v1 + ' ' + v2)\n",
    "            root_words.append({'root':v1, 'remove':v2})\n",
    "            break\n",
    "root_words = pd.DataFrame(root_words, columns=['root','remove'])\n",
    "# manual modify certain words\n",
    "root_words.loc[root_words['root']=='additional', 'root'] = 'addition'\n",
    "root_words.loc[root_words['root']=='clinical', 'root'] = 'clinic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# remove conjugated word variants from training and testing sets\n",
    "for i in range(len(root_words)):\n",
    "    root_word = root_words.loc[i,'root']\n",
    "    remove_word = root_words.loc[i,'remove']\n",
    "    train_df['Abstract'] = train_df['Abstract'].str.replace(remove_word,root_word)\n",
    "    test_df['Abstract'] = test_df['Abstract'].str.replace(remove_word,root_word)\n",
    "    vocab.remove(remove_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After lemmatization, we retain a vocabulary of around 700 words. In the training and testing dataframes, we will now create a column for each vocabulary word and track the number of occurences of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "726"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py:845: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# create a column for each vocab word in training and testing set\n",
    "for v in vocab: # initialize at 0 for all words\n",
    "    train_df.loc[:,v] = 0\n",
    "    test_df.loc[:,v] = 0\n",
    "for index, row in train_df.iterrows():\n",
    "    words = row['Abstract'].split()\n",
    "    for w in words:\n",
    "        if w in vocab:\n",
    "            train_df.loc[index,w] = train_df.loc[index,w] + 1\n",
    "for index, row in test_df.iterrows():\n",
    "    words = row['Abstract'].split()\n",
    "    for w in words:\n",
    "        if w in vocab:\n",
    "            test_df.loc[index,w] = test_df.loc[index,w] + 1\n",
    "            \n",
    "# let's pickle this for later use as well\n",
    "train_df.to_pickle('../Data/pubmed/naivebayes_train_df.pkl')\n",
    "test_df.to_pickle('../Data/pubmed/naivebayes_test_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in the first entry of the training dataset, the abstract contains 2 occurences of `increase`, 0 occurence of `while`, 0 occurence of `cardiovascular`. In the second row, the abstract contains 0 occurence of `increase`, 2 occurences of `while` and 1 occurence of `cardiovascular`. Overall, the training dataset has 10007 rows with 728 columns, and the testing dataset has 252 rows with 728 columns (726 vocab words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   negative  explore  inhibitor  obtained  patient  suggests  however  \\\n",
      "0         0        1          0         0        0         0        0   \n",
      "1         0        0          0         0        0         0        1   \n",
      "\n",
      "   administration  period  older  ...  periodontitis  impact  growth  target  \\\n",
      "0               0       0      0  ...              0       0       0       0   \n",
      "1               0       0      0  ...              0       0       0       1   \n",
      "\n",
      "   involved  oxidative  anxiety  database  gender  order  \n",
      "0         0          0        0         0       0      0  \n",
      "1         0          0        0         0       0      0  \n",
      "\n",
      "[2 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df.iloc[0:2,2:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1007, 728)\n",
      "(252, 728)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get some priors for naive bayes. For each topic, we will need to calculate:\n",
    "   * P(topic) `p_topic`, probability of each topic\n",
    "   * P(word | topic) `p_word_given_topic`, probability of each word given the topic\n",
    "\n",
    "Since there are so many vocabulary words (726) compared to the number of manuscript abstracts available, most entries will have P(word | topic) as 0 for most words; multiplication by zero will wipe out other non-zero probabilities of other words. To avoid this situation, we will implement **Laplace Smoothing** with an alpha = 1, which ensures the probability of word given topic is never exactly 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 # Laplace smoothing alpha is 1\n",
    "n_train = len(train_df)\n",
    "topics = list(train_df['Topic'].unique())\n",
    "n_topics = len(topics)\n",
    "p_topic = {}\n",
    "p_word_given_topic = {}\n",
    "for topic in topics:\n",
    "    train_df_topic = train_df[train_df['Topic']==topic]\n",
    "    p_topic[topic] = len(train_df_topic) / n_train # probability of this topic\n",
    "    p_word = {} # probability of each word for this topic\n",
    "    n_words = train_df_topic.iloc[:,2:].sum(axis=1).sum() # number of words for this topic\n",
    "    for word in vocab:\n",
    "        p_word[word] = (train_df_topic[word].sum() + alpha) / (n_words + alpha * n_topics) # Laplace smoothing\n",
    "    p_word_given_topic[topic] = p_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see before, the dataset is only slightly unbalanced, with `inflammation` accounting for 17% and `cancer` accounting for 9% of the manuscripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inflammation': 0.16782522343594836,\n",
       " 'apoptosis': 0.15789473684210525,\n",
       " 'prognosis': 0.14597815292949354,\n",
       " 'obesity': 0.14498510427010924,\n",
       " 'quality of life': 0.13406156901688182,\n",
       " 'depression': 0.15590863952333664,\n",
       " 'cancer': 0.09334657398212512}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we look at the topic `cancer`. Here are the top 10 words (highest `P(word | topic='cancer')`). We see general words like `conclusion` and `studies`, but we also see important topic-related terms such as `tumor` (cancer usually involved growth of a tumor) and `mortality` (most cancers are life-threatening and thus leading to higher mortality rate compared to other diseases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.010524528942454592, 'conclusion'),\n",
       " (0.008996774741130538, 'tumor'),\n",
       " (0.008487523340689187, 'disease'),\n",
       " (0.00814802240706162, 'studies'),\n",
       " (0.00814802240706162, 'among'),\n",
       " (0.007978271940247837, 'clinic'),\n",
       " (0.007808521473434052, 'outcome'),\n",
       " (0.007808521473434052, 'mortality'),\n",
       " (0.007469020539806484, 'specific'),\n",
       " (0.0072992700729927005, 'their')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(((value,key) for (key,value) in p_word_given_topic['cancer'].items()), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try calculating the posterior for the first row in the testing set. Naive Bayes was able to correctly classify this manuscript as topic `obesity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The posterior probabilities are {'inflammation': 1.6945017779849863e-300, 'apoptosis': 0.0, 'prognosis': 2.672776502143779e-287, 'obesity': 1.0196211598166791e-238, 'quality of life': 8.483031499994382e-290, 'depression': 2.6976390033587065e-288, 'cancer': 1.6681406122475038e-293} for each topic.\n",
      "\n",
      "The predicted and observed topic for this row is obesity and obesity.\n"
     ]
    }
   ],
   "source": [
    "row = test_df.iloc[0] # first row\n",
    "p_topic_row = {}\n",
    "for topic in topics:\n",
    "    p = p_topic[topic] # prior of this topic\n",
    "    for word in vocab:\n",
    "        p *= p_word_given_topic[topic][word] ** row[word] # multiply P(word | topic) to power of num_word_occurence\n",
    "    p_topic_row[topic] = p\n",
    "print('The posterior probabilities are {} for each topic.'.format(p_topic_row))\n",
    "row_topic_predicted = max(p_topic_row, key=p_topic_row.get) # topic with maximum posterior\n",
    "row_topic_observed = row['Topic']\n",
    "print('\\nThe predicted and observed topic for this row is {} and {}.'.format(row_topic_predicted, row_topic_observed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the first row was able to generate correct classification of topics, we will apply our Naive Bayes algorithm to the entire testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7579365079365079\n"
     ]
    }
   ],
   "source": [
    "n_correct = 0\n",
    "incorrect = [] # list of incorrect classifications\n",
    "for index, row in test_df.iterrows():\n",
    "    p_topic_row = {}\n",
    "    for topic in topics:\n",
    "        p = p_topic[topic]\n",
    "        for word in vocab:\n",
    "            p *= p_word_given_topic[topic][word] ** row[word]\n",
    "        p_topic_row[topic] = p\n",
    "    row_topic_predicted = max(p_topic_row, key=p_topic_row.get)\n",
    "    row_topic_observed = row['Topic']\n",
    "    if(row_topic_observed == row_topic_predicted):\n",
    "        n_correct += 1 # number of correct predictions\n",
    "    else:\n",
    "        incorrect.append([row_topic_observed, row_topic_predicted]) # observation and prediction for incorrect entries\n",
    "accuracy = n_correct / len(test_df)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we get an accuracy of 76% in our testing dataset, which is impressive for such a simplistic algorithm. To analyze our incorrect results, we can look at entries that are incorrectly classified for an intuition of what went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depression : quality of life      9\n",
      "inflammation : apoptosis          5\n",
      "cancer : apoptosis                5\n",
      "depression : apoptosis            4\n",
      "prognosis : apoptosis             3\n",
      "inflammation : cancer             3\n",
      "depression : obesity              3\n",
      "apoptosis : inflammation          3\n",
      "cancer : depression               3\n",
      "quality of life : depression      2\n",
      "cancer : quality of life          2\n",
      "quality of life : inflammation    2\n",
      "obesity : depression              2\n",
      "cancer : prognosis                2\n",
      "prognosis : cancer                2\n",
      "obesity : inflammation            1\n",
      "depression : prognosis            1\n",
      "prognosis : inflammation          1\n",
      "obesity : prognosis               1\n",
      "apoptosis : cancer                1\n",
      "cancer : inflammation             1\n",
      "inflammation : prognosis          1\n",
      "prognosis : depression            1\n",
      "quality of life : obesity         1\n",
      "obesity : quality of life         1\n",
      "inflammation : obesity            1\n",
      "Name: Combined, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "incorrect_df = pd.DataFrame(incorrect, columns=['Observed','Predicted'])\n",
    "incorrect_df['Combined'] = incorrect_df['Observed'] + ' : ' + incorrect_df['Predicted']\n",
    "print(incorrect_df['Combined'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Firstly, we see that apoptosis appeared in 4 out of 5 top incorrect labels. Topics `cancer` and `apoptosis` are very similar, because cancer is uncontrolled cell growth whereas apoptosis is the cell death that follows. Topics `inflammation` and `apoptosis` are also very similar because cellular inflammation is followed by cell death. Quite likely, a number of pathologies involve the death of cells, so `apoptosis` might be too general a topic to include.\n",
    "\n",
    "Secondly, the most frequent incorrect classification is between `depression` and `quality of life`. In hindsight, we should have noticed that these two topics are very closely related, as depression and other mental issues very likely lowers the quality of life. Another round of topic identification to remove these overlapping topics can be key to improve the performance of our model.\n",
    "\n",
    "In conclusion, in this work, we use Naive Bayes to classify scientific text abstracts to different topics. The text is processed by removing stop-words, lemmatization and removing words that appear too frequently or too sparsely throughout the entries. Future work make use of more NLP libraries can help optimize this process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
